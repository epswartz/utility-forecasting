{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard Lib\n",
    "from datetime import datetime\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "\n",
    "# UI and Charts\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as tick\n",
    "import ipyvuetify as v\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import FileLink\n",
    "from IPython.display import Javascript\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# General Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "# Modeling and ML\n",
    "import sesd\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Disable Warnings\n",
    "import warnings\n",
    "\n",
    "# Local packages\n",
    "from cleaning import anomalies as anom\n",
    "from cleaning import aggregation as agg\n",
    "from cleaning import remove_nans\n",
    "\n",
    "from models import sarima\n",
    "from models import holt_winters\n",
    "from models import simple_averaging\n",
    "from models import CNN\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") #UI can get messed up if any warnings come up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and option definitions\n",
    "\n",
    "time_col='dt'\n",
    "v.theme.dark = False\n",
    "\n",
    "# Aggregation Levels\n",
    "AGG_LEVEL_DAY = \"D\"\n",
    "AGG_LEVEL_MONTH = \"M\"\n",
    "AGG_LEVEL_WEEK = \"W\"\n",
    "\n",
    "# Constants\n",
    "DAYS_PER_YEAR = 365\n",
    "MONTHS_PER_YEAR = 12\n",
    "WEEKS_PER_YEAR = 52\n",
    "\n",
    "# This list was built from all of the different date formats that were in \n",
    "# Duke FMD's original data - If you need more, you can add it here.\n",
    "fmt_strings = [\n",
    "    \"%m/%d/%y %I:%M %p\",\n",
    "    \"%y-%m-%d\",\n",
    "    \"%y/%m/%d\",\n",
    "    \"%y-%m\",\n",
    "    \"%y/%m\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uploader = ipw.FileUpload(\n",
    "    accept='.csv',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
    "    multiple=False,  # True to accept multiple files upload else False\n",
    "    description=\"Click to Upload\"\n",
    ")\n",
    "\n",
    "\n",
    "# the items are read in from file column headers\n",
    "series_picker_items = [\"Please Upload Data\"]\n",
    "series_picker = v.Select(\n",
    "    items=series_picker_items,\n",
    "    v_model=series_picker_items[0],\n",
    "    label=\"Utility/Building\",\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "#user selects what type of anomaly detection methods they would like to perform\n",
    "anom_multipicker = v.Select(\n",
    "    multiple=True,\n",
    "    outlined=True,\n",
    "    chips=True,\n",
    "    v_model=[],\n",
    "    items=['Nonlocal IQR', 'Rolling Standard Deviation (SD)', 'SESD'],\n",
    "    label='Data Cleaning',\n",
    ")\n",
    "\n",
    "#user enters the multiplier for iqr, if they chose to do iqr anomaly detection\n",
    "iqr_multiplier = v.TextField(\n",
    "    type='Number',\n",
    "    style_='display: none',\n",
    "    outlined=True,\n",
    "    v_model=3,\n",
    "    label='IQR multiplier:',\n",
    ")\n",
    "\n",
    "#user enters the multiplier for sd, if they chose to do sd anomaly detection\n",
    "sd_multiplier = v.TextField(\n",
    "    type='Number',\n",
    "    style_='display: none',\n",
    "    outlined=True,\n",
    "    v_model=3,\n",
    "    label='SD multiplier:',\n",
    ")\n",
    "\n",
    "#user enters the multiplier for sesd, if they chose to do sesd anomaly detection\n",
    "sesd_multiplier = v.TextField(\n",
    "    type='Number',\n",
    "    style_='display: none',\n",
    "    outlined=True,\n",
    "    v_model=3,\n",
    "    label='SESD multiplier:',\n",
    ")\n",
    "\n",
    "#button to display anomaly detection\n",
    "display_anom_btn = v.Btn(\n",
    "    color=\"success\",\n",
    "    class_='ma-2',\n",
    "    children=[\n",
    "        v.Icon(left=True, children=[\n",
    "            'mdi-chart-timeline-variant'\n",
    "        ]),\n",
    "        'Display Data Cleaning'\n",
    "    ],\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "#button to export anomaly detection\n",
    "export_anom_btn = v.Btn(\n",
    "    color=\"primary\",\n",
    "    class_='ma-2',\n",
    "    children=[\n",
    "        v.Icon(left=True, children=[\n",
    "            'mdi-content-save'\n",
    "        ]),\n",
    "        'Export Cleaned Data'\n",
    "    ],\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "#user selects what type of forecasting they would like to perform\n",
    "model_picker = v.Select(\n",
    "    v_model='Average of Previous Years',\n",
    "    items=[\n",
    "        'Average of Previous Years',\n",
    "        'SARIMA',\n",
    "        'Holt-Winters Exponential Smoothing',\n",
    "        'Convolutional Neural Network'\n",
    "    ],\n",
    "    label=\"Forecasting Model\",\n",
    ")\n",
    "\n",
    "#user selects the frequency of the given data\n",
    "freq_data_picker = v.Select(\n",
    "    v_model='Daily',\n",
    "    items=[\n",
    "        '15 Minute Data',\n",
    "        'Daily',\n",
    "        'Weekly',\n",
    "        'Monthly'\n",
    "    ],\n",
    "    label=\"Frequency of Input Data\",\n",
    ")\n",
    "\n",
    "#user enters how much data they would like to forecast for\n",
    "forecast_len_field = v.TextField(\n",
    "    type='Number',\n",
    "    outlined=True,\n",
    "    v_model=24,\n",
    "    label='Forecast Length (Months):',\n",
    ")\n",
    "\n",
    "#button to display forecast\n",
    "display_btn = v.Btn(\n",
    "    color=\"success\",\n",
    "    class_='ma-2',\n",
    "    children=[\n",
    "        v.Icon(left=True, children=[\n",
    "            'mdi-chart-timeline-variant'\n",
    "        ]),\n",
    "        'Display Forecast'\n",
    "    ],\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "#button to export the data to a csv\n",
    "export_btn = v.Btn(\n",
    "    color=\"primary\",\n",
    "    class_='ma-2',\n",
    "    children=[\n",
    "        v.Icon(left=True, children=[\n",
    "            'mdi-content-save'\n",
    "        ]),\n",
    "        'Export Forecast CSV'\n",
    "    ],\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "#button to clear the plots currently on the screen\n",
    "clear_btn = v.Btn(\n",
    "    color=\"error\",\n",
    "    class_='ma-2',\n",
    "    children=[\n",
    "        v.Icon(left=True, children=[\n",
    "            'mdi-delete-forever'\n",
    "        ]),\n",
    "        'Clear Output'\n",
    "    ]\n",
    ")\n",
    "\n",
    "figure_display_area = ipw.Output(\n",
    "    layout={\n",
    "        'align-items':'center'\n",
    "    },\n",
    ")\n",
    "\n",
    "upload_caption_area = ipw.Output(\n",
    "    layout={\n",
    "        'align-items':'left'\n",
    "    }\n",
    ")\n",
    "\n",
    "hidden_dl_link_area = ipw.Output()\n",
    "\n",
    "# All the plots and output go here\n",
    "display_col = v.Col(\n",
    "    tag='div',\n",
    "    cols=9,\n",
    "    children=[\n",
    "        figure_display_area\n",
    "    ]\n",
    ")\n",
    "\n",
    "# All the controls are in one column\n",
    "upload_caption = \"\"\n",
    "controls_col = v.Col(\n",
    "    tag='div',\n",
    "    cols=3,\n",
    "    children=[\n",
    "        freq_data_picker,\n",
    "        series_picker,\n",
    "        anom_multipicker,\n",
    "        iqr_multiplier, \n",
    "        sd_multiplier, \n",
    "        sesd_multiplier,\n",
    "        display_anom_btn,\n",
    "        export_anom_btn,\n",
    "        model_picker,\n",
    "        forecast_len_field,\n",
    "        display_btn,\n",
    "        export_btn,\n",
    "        clear_btn\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Everything together in a row\n",
    "full_display = v.Row(\n",
    "    tag='div',\n",
    "    fluid=True,\n",
    "    children=[\n",
    "        controls_col,\n",
    "        display_col\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper/business logic functions\n",
    "\n",
    "def csv_download_link(df, csv_file_name):\n",
    "    \"\"\"\n",
    "    Display a download link to load a data frame as csv from within a Jupyter notebook\n",
    "    \"\"\"\n",
    "    global fl\n",
    "    \n",
    "    df.to_csv(csv_file_name, index=True, index_label=time_col)\n",
    "    fl = FileLink(csv_file_name)\n",
    "    with hidden_dl_link_area:\n",
    "        #display(Javascript('window.open(\"{url}\");'.format(url=fl.path)))\n",
    "        display(fl)\n",
    "    # TODO Cleanup old files, if that's important to anyone\n",
    "    \n",
    "def clean_data(forecast_key, anom_methods, freq_data, iqr_mult, sd_mult, sesd_mult):\n",
    "    \"\"\"\n",
    "    Handles data cleaning and returns result along with the variables needed to plot the anomalies\n",
    "    Doesn't take any params, as it accesses information from globals which are edited by various UI functions.\n",
    "    \"\"\"\n",
    "    global util_data\n",
    "    \n",
    "    # Get correct period and letter frequency based on the frequency the user gave\n",
    "    period, freq=get_period_freq(freq_data)\n",
    "    \n",
    "    # Create a new dataframe with just the series of interest (only first column)\n",
    "    current_series_df = util_data[[time_col, forecast_key]].copy()\n",
    "    current_series_df = current_series_df.set_index(time_col)\n",
    "    \n",
    "    # Aggregate data into daily if 15 min data\n",
    "    if freq_data=='15 Minute Data':\n",
    "        current_series_df = agg.aggregate(current_series_df, AGG_LEVEL_DAY)\n",
    "    \n",
    "    #remove the nans at the beginning of the data and make the rest of the nans 0\n",
    "    #common for Duke's utility data to have nans at the beginning before the data starts actually being recorded\n",
    "    current_series_df = remove_nans.drop_beg_nans_rest_0(current_series_df)\n",
    "    \n",
    "    #if the length of the data isn't 2 periods, throw error\n",
    "    if len(current_series_df[current_series_df.columns[0]])<=2*period:\n",
    "        raise ValueError(f\"ERROR: Length of your data (not including blanks) for {forecast_key.strip()} must be greater than 2 years. You may also have the Input Frequency set wrong.\")\n",
    "\n",
    "    doSesd = 'SESD' in anom_methods\n",
    "    doSD = 'Rolling SD' in anom_methods\n",
    "    doIQR = 'Nonlocal IQR' in anom_methods\n",
    "    \n",
    "    # If the user wants to do any anomaly detection method, do it\n",
    "    if doSesd or doSD or doIQR:\n",
    "        cleaned_current_series_df,sesd_anoms,sd_anoms,iqr_anoms,iqr_upper_bound,percent_anomalies=\\\n",
    "            anom.removeAnomaliesAndImpute(\n",
    "            current_series_df,\n",
    "            doSesd,\n",
    "            doSD,\n",
    "            doIQR,\n",
    "            period, \n",
    "            iqr_mult, \n",
    "            sd_mult,\n",
    "            sesd_mult\n",
    "        )\n",
    "        return cleaned_current_series_df,forecast_key,current_series_df,sesd_anoms,sd_anoms,iqr_anoms,iqr_upper_bound,\\\n",
    "                            percent_anomalies\n",
    "    else:\n",
    "        return current_series_df,forecast_key,current_series_df,None,None,None,None,None\n",
    "    \n",
    "def get_period_freq(freq_data):\n",
    "    \"\"\"\n",
    "    Given the word frequency (\"daily\", \"monthly\", etc.) of the data, returns the period and the \n",
    "    frequency of the data where frequency is a single character ('D', 'W', or 'M')\n",
    "    \"\"\"\n",
    "    period=None\n",
    "    freq=None\n",
    "\n",
    "    #determine what the period and letter frequency should be based on the given frequency of the user\n",
    "    if freq_data=='15 Minute Data' or freq_data=='Daily':\n",
    "        period=DAYS_PER_YEAR\n",
    "        freq=AGG_LEVEL_DAY\n",
    "    elif freq_data=='Weekly':\n",
    "        period=WEEKS_PER_YEAR\n",
    "        freq=AGG_LEVEL_WEEK\n",
    "    elif freq_data=='Monthly':\n",
    "        period=MONTHS_PER_YEAR\n",
    "        freq=AGG_LEVEL_MONTH\n",
    "        \n",
    "    return period, freq\n",
    "    \n",
    "def agg_data(cleaned_current_series_df):\n",
    "    \"\"\"\n",
    "    Given a cleaned, daily dataframe from the data cleaning function, aggregates that data to monthly.\n",
    "    Can do this regardless of the frequency of the data (weekly, daily, and even monthly)\n",
    "    We do this since we do all forecasting at the monthly level\n",
    "    \"\"\"\n",
    "    monthly_df = agg.aggregate(cleaned_current_series_df, AGG_LEVEL_MONTH)\n",
    "    \n",
    "    # Double check greater than 2 years (24 months) data after aggregation to monthly level\n",
    "    if len(monthly_df['data'])<=24:\n",
    "        raise ValueError('Length of your data (not including blanks) must be greater than 2 periods')\n",
    "\n",
    "    ts = monthly_df.iloc[:, 0].to_numpy()\n",
    "    return monthly_df\n",
    "\n",
    "def create_forecast(model, monthly_df):\n",
    "    \"\"\"\n",
    "    Creates the forecasting data, \n",
    "    \"\"\"\n",
    "    global current_forecast_ts\n",
    "    \n",
    "    period = MONTHS_PER_YEAR\n",
    "    freq = AGG_LEVEL_MONTH\n",
    "    error_start_date,error_end_date=None, None\n",
    "    forecast_length = int(forecast_len_field.v_model)\n",
    "    \n",
    "    if model_picker.v_model=='SARIMA':\n",
    "        predictions,mape,mase,error_start_date,error_end_date=\\\n",
    "            sarima.forecast(data=monthly_df, freq=freq, period=period, numPointsPredict=forecast_length)\n",
    "        \n",
    "        #if predictions has a nan, then that means sarima model doesn't work for the given data (rarely happens)\n",
    "        if np.isnan(predictions.values).any():\n",
    "            raise ValueError(\"ERROR: SARIMA forecasting unable to be completed for this dataset.\")\n",
    "        \n",
    "    elif model_picker.v_model=='Average of Previous Years':\n",
    "        #get the results from the simple averaging model\n",
    "        predictions,mape,error_start_date,error_end_date=simple_averaging.forecast(data=monthly_df,period=period,\\\n",
    "                                                        freq=freq,numPointsPredict=forecast_length)\n",
    "        mase = 1 # by definition, because this is the naive model\n",
    "\n",
    "    elif model_picker.v_model=='Holt-Winters Exponential Smoothing':\n",
    "        #get the results from the holt winters model\n",
    "        predictions,mape,mase,error_start_date,error_end_date=holt_winters.forecast(data=monthly_df,\\\n",
    "                            period=period,freq=freq,numPointsPredict=forecast_length)\n",
    "    \n",
    "    elif model_picker.v_model=='Convolutional Neural Network':\n",
    "        #get the results from the CNN model\n",
    "        predictions,mase,mape,error_start_date,error_end_date=CNN.predict_with_CNN(monthly_df,\\\n",
    "                                                                                   forecast_length,\\\n",
    "                                                                                   verbose=False)\n",
    "        # Convert np array to series with time index\n",
    "        predictions = pd.Series(predictions.reshape((forecast_length,)))\n",
    "        predictions.index = pd.date_range(start=monthly_df.index[-1] , periods= forecast_length + 1, freq = 'M')[1:]\n",
    "    \n",
    "    return predictions, mape, mase, error_start_date, error_end_date\n",
    "\n",
    "\n",
    "def handle_output():\n",
    "    \"\"\"\n",
    "    This function is run by every button handler that produces output to the screen. This does the checking\n",
    "    to see if we need any new computation (new cleaning of data and/or new forecasts) and then returns its results.\n",
    "    \n",
    "    The main handlers then access that data as they need to.\n",
    "    \"\"\"\n",
    "    #global variables\n",
    "    global current_forecast_key\n",
    "    global current_forecast_ts\n",
    "    global current_anom_methods\n",
    "    global current_aggregated_data\n",
    "    global current_forecasting_method\n",
    "    global current_aggregation_level\n",
    "    global new_upload\n",
    "    global util_data\n",
    "    global current_iqr_mult\n",
    "    global current_sd_mult\n",
    "    global current_sesd_mult\n",
    "    \n",
    "    # First check if we need to do anything.\n",
    "    # Sort anom method lists so they can be compared\n",
    "    # if new upload is true, it means new data was uploaded (however other fields may not have changed,\n",
    "    #but need to redo calculations)\n",
    "    anom_multipicker.v_model.sort()\n",
    "    current_anom_methods.sort()\n",
    "    \n",
    "    #boolean that checks if anything has changed with regard to data cleaning (data being forecasted, type of anomaly \n",
    "    #detection, anomaly multipliers or the aggregation level), run again\n",
    "    bool_changes_anomaly = current_forecast_key != series_picker.v_model or current_anom_methods != anom_multipicker.v_model\\\n",
    "        or current_aggregation_level != freq_data_picker.v_model or new_upload or\\\n",
    "        current_iqr_mult != iqr_multiplier.v_model or current_sd_mult != sd_multiplier.v_model or\\\n",
    "        current_sesd_mult != sesd_multiplier.v_model\n",
    "    \n",
    "    #redo if changes to anomaly detection\n",
    "    if bool_changes_anomaly:\n",
    "        results = clean_data(series_picker.v_model,anom_multipicker.v_model,freq_data_picker.v_model,\\\n",
    "                                               float(iqr_multiplier.v_model), float(sd_multiplier.v_model),\\\n",
    "                                               float(sesd_multiplier.v_model))\n",
    "        cleaned_current_series_df = results[0] #only care about the first thing returned\n",
    "        \n",
    "        #aggregate to monthly level because that is how we do our forecasting\n",
    "        current_aggregated_data = agg_data(cleaned_current_series_df)\n",
    "    \n",
    "    #always just forecast graph again even if no changes\n",
    "    new_upload=False\n",
    "\n",
    "    with figure_display_area:\n",
    "        print(\"Calculating Forecast...\")\n",
    "\n",
    "    predictions, mape, mase, error_start_date, error_end_date =\\\n",
    "        create_forecast(model_picker.v_model, current_aggregated_data)\n",
    "    #reassign all of the global variables\n",
    "    current_forecast_ts = predictions\n",
    "    current_forecast_key = series_picker.v_model\n",
    "    current_anom_methods = anom_multipicker.v_model\n",
    "    current_forecasting_method = model_picker.v_model\n",
    "    current_aggregation_level = freq_data_picker.v_model\n",
    "    current_iqr_mult = float(iqr_multiplier.v_model)\n",
    "    current_sd_mult = float(sd_multiplier.v_model)\n",
    "    current_sesd_mult = float(sesd_multiplier.v_model)\n",
    "    \n",
    "    # Save our data in case they call again with the same stuff\n",
    "    \n",
    "    return current_forecast_ts, mape, mase, error_start_date, error_end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Graphics/UI and direct handler functions\n",
    "\n",
    "def render():\n",
    "    \"\"\"\n",
    "    Becuase the content of some UI elements depends on the settings in others,\n",
    "    the UI needs to be re-displayed whenever something is changed.\n",
    "    \n",
    "    This gets called once at program start, and then again after every setting change.\n",
    "    \"\"\"\n",
    "    \n",
    "    #display(util_picker) # No util picker for demo.\n",
    "    display(file_uploader)\n",
    "    display(upload_caption_area)\n",
    "    display(hidden_dl_link_area)\n",
    "    display(full_display)\n",
    "\n",
    "def on_change():\n",
    "    \"\"\"\n",
    "    Called whenever any UI element is changed.\n",
    "    \n",
    "    Responsible for changing active_series, an identifier (file and column name) for the series of interest.\n",
    "    Also responsible for calling render().\n",
    "    \"\"\"\n",
    "    render()\n",
    "    \n",
    "    \n",
    "# TODO split the data processing part of this out into something that gets called by this and\n",
    "# the export handler.\n",
    "def display_forecast_on_click(widget, event, data):\n",
    "    \"\"\"\n",
    "    Runs whenever the \"Display Forecast\" button is clicked.\n",
    "    \n",
    "    Does the forecasting if needed, and show the plot.\n",
    "    \"\"\"\n",
    "    global current_forecast_key\n",
    "    global current_forecast_ts\n",
    "    global current_aggregated_data\n",
    "    global util_data\n",
    "    \n",
    "    try:\n",
    "        predictions, mape, mase, error_start_date, error_end_date = handle_output()\n",
    "\n",
    "        show_forecast_plot(predictions, mape, mase, error_start_date, error_end_date)\n",
    "    except Exception as e:\n",
    "        with figure_display_area:\n",
    "            print(\"An error occurred during forecasting:\\n\", str(e))\n",
    "\n",
    "def export_btn_on_click(widget, event, data):\n",
    "    global current_forecast_ts\n",
    "    \"\"\"\n",
    "    Construct export dataframe\n",
    "    \n",
    "    This also stores into current_aggregated_data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predictions, mape, mase, error_start_date, error_end_date = handle_output() \n",
    "\n",
    "        dti = pd.date_range(current_aggregated_data.index[-1], periods=len(current_forecast_ts)+1, freq='M')[1:]\n",
    "        forecast_df = pd.DataFrame(index=dti, data={\"data\":current_forecast_ts.to_numpy()})\n",
    "        export_df = pd.concat([current_aggregated_data,forecast_df])\n",
    "        \n",
    "        #rename the data column to the forecast key\n",
    "        export_df=export_df.rename(columns={\"data\":series_picker.v_model})\n",
    "\n",
    "        now = datetime.now()\n",
    "        time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        with figure_display_area:\n",
    "            print(\"Creating download link above...\")\n",
    "        csv_download_link(export_df, f\"forecast_{time_str}.csv\")\n",
    "    except Exception as e:\n",
    "        with figure_display_area:\n",
    "            print(\"An error occurred during forecasting:\\n\", str(e))\n",
    "            \n",
    "def display_anom_on_click(widget, event, data):\n",
    "    \"\"\"\n",
    "    Runs whenever the \"Display Anomaly Detection\" button is clicked.\n",
    "    \n",
    "    Only can click if one of anomaly detection methods selected\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        cleaned_current_series_df,forecast_key,current_series_df,sesd_anoms,sd_anoms,iqr_anoms,iqr_upper_bound,\\\n",
    "                            percent_anomalies=clean_data(series_picker.v_model,anom_multipicker.v_model,\\\n",
    "                                                         freq_data_picker.v_model,float(iqr_multiplier.v_model),\\\n",
    "                                                         float(sd_multiplier.v_model),float(sesd_multiplier.v_model))\n",
    "        \n",
    "        show_anomalies_plot(forecast_key,current_series_df,sesd_anoms,sd_anoms,iqr_anoms,iqr_upper_bound,\\\n",
    "                            percent_anomalies)\n",
    "    except Exception as e:\n",
    "        with figure_display_area:\n",
    "            print(\"An error occurred during anomaly detection:\\n\", str(e))\n",
    "            \n",
    "def export_anom_btn_on_click(widget, event, data):\n",
    "    \"\"\"\n",
    "    Runs the clean data function and exports the result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results=clean_data(series_picker.v_model,anom_multipicker.v_model,freq_data_picker.v_model,\\\n",
    "                           float(iqr_multiplier.v_model),float(sd_multiplier.v_model),float(sesd_multiplier.v_model))\n",
    "        cleaned_current_series_df = results[0] #only care about the first thing returned \n",
    "        \n",
    "        #rename the data column to the forecast key\n",
    "        cleaned_current_series_df=cleaned_current_series_df.rename(columns={\"data\":series_picker.v_model})\n",
    "\n",
    "        now = datetime.now()\n",
    "        time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        with figure_display_area:\n",
    "            print(\"Creating download link above...\")\n",
    "        csv_download_link(cleaned_current_series_df, f\"anom_detection_{time_str}.csv\")\n",
    "    except Exception as e:\n",
    "        with figure_display_area:\n",
    "            print(\"An error occurred during forecasting:\\n\", str(e))\n",
    "        \n",
    "# Have to wrap the clear fn to take care of the param\n",
    "def clear_btn_on_click(widget, event, data):\n",
    "    plt.close('all')\n",
    "    figure_display_area.clear_output()\n",
    "    hidden_dl_link_area.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for plotting\n",
    "\n",
    "def y_fmt(tick_val, pos):\n",
    "    if tick_val > 1000000:\n",
    "        val = int(tick_val)/1000000\n",
    "        return f'{val} M'\n",
    "    elif tick_val > 1000:\n",
    "        val = int(tick_val) / 1000\n",
    "        return f'{val} k'\n",
    "    else:\n",
    "        return tick_val\n",
    "\n",
    "def show_forecast_plot(predictions,mape,mase,error_start_date,error_end_date):\n",
    "    \"\"\"\n",
    "    Plots the forecasting for the required model. \n",
    "    Given the predictions, mean absolute precent error, mean absolute scaled error,\n",
    "    the start date for the error and the end date for the error of the testing data\n",
    "    Uses global variable current_aggregated_data and figure_display_area\n",
    "    \"\"\"\n",
    "    with figure_display_area:\n",
    "        #ts is an array of the previous data and x is used for graphing and is a range 0-length of ts of indices\n",
    "        ts=current_aggregated_data[current_aggregated_data.columns[0]]\n",
    "        x=np.asarray(range(len(predictions)+len(ts)-1)) #-1 because one point overlap\n",
    "        x_start = 0\n",
    "        x_end = len(x)-1\n",
    "        x_len = x_end - x_start\n",
    "        \n",
    "        #create equal spaces apart to put ticks with dates\n",
    "        xt = [x_start, x_start + (x_len//3), x_start+(2*(x_len//3)), x_end]\n",
    "        xtl=[]\n",
    "        for curIndex in xt: \n",
    "            if curIndex<len(ts):\n",
    "                xtl.append(pd.Timestamp.date(current_aggregated_data.index[curIndex]))\n",
    "            else:\n",
    "                xtl.append(pd.Timestamp.date(predictions.index[curIndex-len(ts)+1]))\n",
    "            \n",
    "        plt.figure(figsize=(12,8))\n",
    "        labels_and_colors=[]\n",
    "        plt.title(\"%s forecasting: %s\" % (model_picker.v_model,current_forecast_key),size=15)\n",
    "        \n",
    "        #plot the training data\n",
    "        labels_and_colors.append([\"Training Data\", \"blue\"])\n",
    "        plt.plot(x[0:len(ts)], ts, label=labels_and_colors[-1][0], color=labels_and_colors[-1][1], marker=\"o\")\n",
    "        \n",
    "        #forecast one more than needed because forecast the last value of the ts again\n",
    "        labels_and_colors.append([\"Forecast\", \"orange\"])\n",
    "        plt.plot(x[len(ts)-1:], predictions, color=labels_and_colors[-1][1], label=labels_and_colors[-1][0], marker=\"o\")\n",
    "        \n",
    "        #set the ticks, names and ranges for the x and y axis\n",
    "        plt.xticks(xt, xtl, size=15)\n",
    "        plt.xlim(x_start, x_end)\n",
    "        plt.ylabel(\"Utility Usage\", size=15)\n",
    "        plt.xlabel(\"Date\", size=15)\n",
    "        plt.grid(alpha=.3)\n",
    "        \n",
    "        #set the legend outside the graph window and set the text inside it to show the eror\n",
    "        handles=[]\n",
    "        for label_color_pair in labels_and_colors:\n",
    "            handles.append(mpatches.Patch(color=label_color_pair[1], label=label_color_pair[0]))\n",
    "        error=\"MAPE: %.2f%%\\nMASE: %.2f\\nError computed over:\\n%s to %s\"\\\n",
    "            % (mape,mase,error_start_date,error_end_date) \n",
    "        handles.append(mpatches.Patch(color='none', label=error))\n",
    "        plt.legend(handles=handles, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0., fontsize=12)\n",
    "        ax = plt.gca()\n",
    "        ax.yaxis.set_major_formatter(tick.FuncFormatter(y_fmt))\n",
    "        plt.show()\n",
    "        \n",
    "def show_anomalies_plot(forecast_key, cleaned_current_series_df, sesd_anoms, sd_anoms, iqr_anoms, iqr_upper_bound,\\\n",
    "                        percent_anomalies):\n",
    "    \"\"\"\n",
    "    Plot the anomalies if the user chose to remove anomalies\n",
    "    Uses global variable figure_display_area\n",
    "    \"\"\"\n",
    "    with figure_display_area:\n",
    "        #ts is an array of the previous data and x is used for graphing and is a range of indices\n",
    "        ts=cleaned_current_series_df[cleaned_current_series_df.columns[0]]\n",
    "        x = np.asarray(range(len(ts)))\n",
    "        x_start = 0\n",
    "        x_end = len(x)-1\n",
    "        x_len = x_end - x_start\n",
    "        labels_and_colors=[] #keep track of this for making legend\n",
    "        \n",
    "        #create equal spaces apart to put ticks with dates\n",
    "        xt = [x_start, x_start + (x_len//3), x_start+(2*(x_len//3)), x_end]\n",
    "        xtl=[]\n",
    "        for index in xt: \n",
    "            #xtl.append(pd.Timestamp.date(cleaned_current_series_df.index[index]))\n",
    "            xtl.append(cleaned_current_series_df.index[index])\n",
    "\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.title(\"Anomalies for {}\".format(forecast_key), size=20)\n",
    "        plt.plot(x, ts, color=\"#7ea4ce\", label=\"Original Usage\") #original data\n",
    "        \n",
    "        #only plot the anomalies boolean array if not none which means the user selected to have that anomaly\n",
    "        #detection method done\n",
    "        if iqr_anoms!=None:\n",
    "            labels_and_colors.append([\"IQR upper boundary\", \"orange\", \"dashed\"])\n",
    "            plt.plot(x, iqr_upper_bound, label=labels_and_colors[-1][0], color=labels_and_colors[-1][1],\\\n",
    "                     linestyle=labels_and_colors[-1][2])\n",
    "            labels_and_colors.append([\"IQR anomalies\", \"green\", None])\n",
    "            plt.scatter(x[iqr_anoms],ts[iqr_anoms], label=labels_and_colors[-1][0], color=labels_and_colors[-1][1],\\\n",
    "                        zorder = 5)\n",
    "        if sd_anoms!=None:\n",
    "            labels_and_colors.append([\"SD anomalies\", \"red\", None])\n",
    "            plt.scatter(x[sd_anoms],ts[sd_anoms], label=labels_and_colors[-1][0], color=labels_and_colors[-1][1],\\\n",
    "                        zorder = 5)\n",
    "        if sesd_anoms!=None:\n",
    "            labels_and_colors.append([\"Seasonal anomalies\", \"blue\", None])\n",
    "            plt.scatter(x[sesd_anoms],ts[sesd_anoms], label=labels_and_colors[-1][0], color=labels_and_colors[-1][1],\\\n",
    "                        zorder = 5)\n",
    "        if sesd_anoms!=None and sd_anoms!=None:\n",
    "            labels_and_colors.append([\"SD & Seasonal anomalies\", \"purple\", None])\n",
    "            plt.scatter(x[sesd_anoms and sd_anoms],ts[sesd_anoms and sd_anoms], label=labels_and_colors[-1][0],\\\n",
    "                        color=labels_and_colors[-1][1], zorder = 5)\n",
    "        \n",
    "        #set the ticks, names and ranges for the x and y axis\n",
    "        plt.xticks(xt, xtl, size=15)\n",
    "        plt.xlim(x_start, x_end)\n",
    "        plt.yticks(size=15)\n",
    "        plt.ylim()\n",
    "        plt.xlabel(\"Date\", size=15)\n",
    "        plt.ylabel(\"Utility Usage\", size=15)\n",
    "        plt.grid(alpha=.3)\n",
    "        \n",
    "        #set the legend outside the graph window and set the text inside it to show the percent anomalies\n",
    "        handles=[]\n",
    "        for label_color_pair in labels_and_colors:\n",
    "            handles.append(mpatches.Patch(label=label_color_pair[0], color=label_color_pair[1],\\\n",
    "                                          linestyle=label_color_pair[2]))\n",
    "        anomalies=\"Percent data marked\\nanomalous: %.2f%%\" % (percent_anomalies) \n",
    "        handles.append(mpatches.Patch(color='none', label=anomalies))\n",
    "        plt.legend(handles=handles, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0., fontsize=12)\n",
    "        ax = plt.gca()\n",
    "        ax.yaxis.set_major_formatter(tick.FuncFormatter(y_fmt))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reader\n",
    "def file_uploader_on_change(change):\n",
    "    global changes\n",
    "    global calls\n",
    "    global util_data\n",
    "    global series_picker\n",
    "    global new_upload\n",
    "    \n",
    "    [filename] = file_uploader.value\n",
    "    uploaded_file = file_uploader.value[filename]\n",
    "    size = uploaded_file[\"metadata\"][\"size\"]\n",
    "    file_uploader.description = filename\n",
    "    \n",
    "    # Read the data in dataframe\n",
    "    util_data = pd.read_csv(io.BytesIO(uploaded_file[\"content\"]))\n",
    "    \n",
    "    # Convert the time column to datetime\n",
    "    for f in fmt_strings:\n",
    "        try:\n",
    "            util_data[time_col] = pd.to_datetime(util_data[time_col], format=f)\n",
    "            break\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # Set names for dropdown in the UI\n",
    "    util_series_names = util_data.drop([time_col], axis=1).columns\n",
    "    series_picker.items = [n for n in util_series_names]\n",
    "    series_picker.v_model = series_picker.items[0]\n",
    "    series_picker.disabled = False\n",
    "    display_btn.disabled = False\n",
    "    export_btn.disabled = False\n",
    "    new_upload = True\n",
    "    \n",
    "    # Print status\n",
    "    upload_caption_area.clear_output()\n",
    "    with upload_caption_area:\n",
    "        print(f\"Loaded: {filename}\\nSize: {size} Bytes\")\n",
    "        \n",
    "        \n",
    "def anom_multipicker_on_change(change):\n",
    "    \"\"\"\n",
    "    Shows/Hides controls for anomaly detection tuning.\n",
    "    \"\"\"\n",
    "    active_anom_types = change.new\n",
    "    \n",
    "    #if a type of anomaly detection has been selected, enable the button to forecast and export\n",
    "    if active_anom_types!=[]:\n",
    "        display_anom_btn.disabled=False\n",
    "        export_anom_btn.disabled=False\n",
    "    else:\n",
    "        display_anom_btn.disabled=True\n",
    "        export_anom_btn.disabled=True\n",
    "        \n",
    "    if 'Nonlocal IQR' in active_anom_types:\n",
    "        iqr_multiplier.style_ = 'display: block'\n",
    "    else:\n",
    "        iqr_multiplier.style_ = 'display: none'\n",
    "    \n",
    "    if 'SESD' in active_anom_types:\n",
    "        sesd_multiplier.style_ = 'display: block'\n",
    "    else:\n",
    "        sesd_multiplier.style_ = 'display: none'\n",
    "        \n",
    "    if 'Rolling SD' in active_anom_types:\n",
    "        sd_multiplier.style_ = 'display: block'\n",
    "    else:\n",
    "        sd_multiplier.style_ = 'display: none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bbccbe7fea4e819d5663686fff0c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv', description='Click to Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc2bf3fc9be4f868b9929885a066b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f628a8f90547d8969d578e041ccc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55a68786c5d4ea6ad2584d42deda51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Row(children=[Col(children=[Select(items=['15 Minute Data', 'Daily', 'Weekly', 'Monthly'], label='Frequency of…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure on_click and on_change behaviors\n",
    "display_btn.on_event('click', display_forecast_on_click)\n",
    "clear_btn.on_event('click', clear_btn_on_click)\n",
    "export_btn.on_event('click', export_btn_on_click)\n",
    "display_anom_btn.on_event('click', display_anom_on_click)\n",
    "export_anom_btn.on_event('click', export_anom_btn_on_click)\n",
    "file_uploader.observe(file_uploader_on_change, 'value')\n",
    "anom_multipicker.observe(anom_multipicker_on_change, 'v_model')\n",
    "\n",
    "# Dummy globals\n",
    "fl = None\n",
    "util_data = None\n",
    "current_forecast_key = \"\"\n",
    "current_forecast_ts = None\n",
    "current_aggregated_data = None\n",
    "current_anom_methods = []\n",
    "current_forecasting_method = None\n",
    "current_aggregation_level = None\n",
    "new_upload = False\n",
    "\n",
    "render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
